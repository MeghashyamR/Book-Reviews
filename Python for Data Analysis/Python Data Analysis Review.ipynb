{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Python for Data Analysis - A Critical Line-by-Line Review\n",
    "\n",
    "In this post, I will offer my review of the book, Python for Data Analysis by Wes McKinney. My name is Ted Petrou and I am an expert at pandas and author of the recently released Pandas Cookbook. I thoroughly read through PDA and created a very long, 50-page review that is available on github. This post provides some of the highlights of the full review.\n",
    "\n",
    "## What is a critical line-by-line review?\n",
    "I read this book as if I was the only technical reviewer and I was counted on to find all the possible errors. Every single line of code was scrutinized and explored to see if a better solution existed. Having spent nearly every day of the last 18 months writing and talking about pandas, I have formed strong opinions about how it should be used. This critical examination lead to me finding fault to quite a large percentage of the code. This resulted in a lengthy 50-page review that would be overwhelming for a general blog post.\n",
    "\n",
    "## Review Focuses on Pandas\n",
    "The main focus of PDA is on the pandas library but it does have material on basic Python, IPython and NumPy, which are covered in chapters 1-4 and in the appendices. The pandas library is covered in chapters 5-14, which will be the primary focus of this review.\n",
    "\n",
    "## Overall Summary of PDA\n",
    "\n",
    "#### PDA is like a Reference Manual\n",
    "PDA is written very much like a reference manual, methodically covering one feature or operation before moving onto the next. The current version of the official documentation is a much more thorough reference guide if you are looking to learn pandas in a similar type of manner.\n",
    "\n",
    "#### Little Data Analysis\n",
    "There is very little actual data analysis and almost no teaching of common techniques or theory that are crucial to making sense of data. \n",
    "\n",
    "#### Uses Randomly Generated Data\n",
    "The vast majority of examples use randomly generated or contrived data that bear little resemblance to what data actually look like in the real world. \n",
    "\n",
    "#### Operations are Learned in Isolation\n",
    "For the most part, the operations are learned in isolation, independent from other parts of the pandas library. This is not how data analysis happens in the real-world. Often, many commands from different sections of the library will be needed to be combined together to get a desired result.\n",
    "\n",
    "\n",
    "#### Is Already Outdated\n",
    "Although the commands will will work for the current pandas version 0.21, it is clear that the book was not updated past version 0.18. This is clear because the `resample` method gained the `on` parameter in version 0.19 which was absent in PDA. The powerful and popular function `merge_asof` was also added in version 0.19 and is not mentioned once in the book.\n",
    "\n",
    "#### Lots of Non-Modern and Non-Idiomatic Code\n",
    "There were numerous instances were it was clear that book was not updated to show more modern code. For instance, the `take` method is almost never used and completely replaced by the `iloc` indexer.\n",
    "\n",
    "#### Indexing Confusion\n",
    "One of the most confusing things for newcomers to pandas are the multiple ways to select data with the indexing operator `[]`, `.loc` and `.iloc`. There is not enough detailed explanations for the reader to walk away with a thorough understanding of each.\n",
    "\n",
    "# Chapter-by-Chapter Review\n",
    "In this next section I will give short summaries of each chapter followed by more details on specific code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Getting Started with pandas\n",
    "\n",
    "\n",
    "### What it covers\n",
    "* construction of Series and DataFrames\n",
    "* basic selection with `[]`, `.loc` and `.iloc`\n",
    "* simple functions/methods like `isnull/notnull`, `head/tail`, `sort_values/sort_index`, `drop`\n",
    "* basic summary methods `describe`, `max`, `mean`, `value_counts` \n",
    "* more complex methods like `reindex`, `apply`, `applymap`, `map`\n",
    "* a pinch of data alignment\n",
    "* new column creation\n",
    "* simple correlation and covariance\n",
    "* simply boolean selection\n",
    "\n",
    "## Chapter 5. Summary:\n",
    "Chapter 5 covers an introduction to the primary pandas data structures, the Series and the DataFrame. The commands in this chapter can almost all be found directly in the pandas official documentation. There is actually more depth in the official documentation. For instance, the book begins by producing a near replica of the intro section of the documentation (http://pandas.pydata.org/pandas-docs/stable/dsintro.html). All of the index selection in the chapter is covered in greater detail in the indexing section of the documentation - http://pandas.pydata.org/pandas-docs/stable/indexing.html. Like most of the book, this chapter uses randomly generated or contrived data that has little application to real data analysis.\n",
    "\n",
    "## Chapter 5 Main Criticisms\n",
    "The book begins by erroneously stating that the Series and DataFrame constructors are used often. They are not, and most data is read directly into pandas as a DataFrame with the `read_csv` function or one of the many other `read_*` functions. I would also not encourage people to import the constructors directly into their global namespace like this:\n",
    "\n",
    "```\n",
    "from pandas import Series, DataFrame\n",
    "```\n",
    "\n",
    "It is much more common to just import pandas by itself and alias it to `pd`\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "#### Series and DataFrame constructors\n",
    "The first real line of code creates a Series with its constructor. I don't think Series and DataFrame construction should be covered first as it's typically not used at all during most data analyses. I also don't like the variable name `obj` to refer to the Series. I like to be more verbose and would refer to it as `ser` or `s`. DataFrames are given the better name `frame` but this is not typical of what is used throughout the pandas world. The documentation uses `s` for Series and `df` for DataFrames. On one occasion, the DataFrame is given the name `data`. Whatever name is used should be consistent.\n",
    "\n",
    "#### Indexing\n",
    "One of the next operations is selection of some data through the indexing operator itself `[]` on a Series.  This is a severe mistake. I highly discourage all pandas users to use the indexing operator by itself to select elements from a Series. It is ambiguous and not at all explicit. Instead, I advise to always use `.loc/.iloc` to make selections from a Series. The only recommended use for the indexing operator is when doing boolean indexing.\n",
    "\n",
    "```\n",
    ">>> s = pd.Series(data=[9, -5, 13], index=['a', 'b', 'c'])\n",
    ">>> s\n",
    "a     9\n",
    "b    -5\n",
    "c    13\n",
    "dtype: int64\n",
    "\n",
    "# The indexing operator can take integers or labels and thus is ambiguous\n",
    "\n",
    ">>> s[1]\n",
    "-5\n",
    "\n",
    ">>> s['b]\n",
    "-5\n",
    "\n",
    "### Should use .iloc for integers and .loc for labels\n",
    "\n",
    ">>> s.iloc[1]\n",
    "-5\n",
    "\n",
    ">>> s.loc['b']\n",
    "5\n",
    "```\n",
    "\n",
    "#### DataFrame style output\n",
    "I also don't like the raw non-stylized output of the DataFrames. Most people will be using Jupyter Notebooks when covering this book and it would make more sense to have the visual display of DataFrames match what they see. It would also be nicer to see a clearly labeled image of the DataFrame and Series components as is done in Pandas Cookbook.\n",
    "\n",
    "DataFrame Anatomy\n",
    "![](images/dataframe_anatomy.png)\n",
    "\n",
    "Series Anatomy\n",
    "\n",
    "![](images/series_anatomy.png)\n",
    "\n",
    "#### apply method\n",
    "I really don't like teaching the `apply` method so soon as beginners almost always misuse it. There is almost always a better and more efficient alternative. In one of the cases, `apply` is used to aggregate by calling a custom function on each row. This is completely unnecessary, as there exists a quick and easy way to do with built-in methods.\n",
    "\n",
    "\n",
    "```\n",
    ">>> df = pd.DataFrame(np.random.randn(4, 3), columns=list('abc'),\n",
    "        index=['Quebec', 'Ontario', 'Alberta', 'Nova Scotia'])\n",
    ">>> df\n",
    "```\n",
    "![](images/05_1_provinces.png)\n",
    "\n",
    "```\n",
    "### misuse of apply\n",
    ">>> df.apply(lambda x: x.max() - x.min())\n",
    "a    2.600356\n",
    "b    0.880358\n",
    "c    2.039398\n",
    "dtype: float64\n",
    "\n",
    "### idiomatically\n",
    ">>> df.max() - df.min()\n",
    "a    2.600356\n",
    "b    0.880358\n",
    "c    2.039398\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "I understand that a contrived example is used here to understand the mechanics of `apply`, but this is exactly how new users get confused. `apply` is an iterative and slow method that should only be used if no other more efficient and vectorized solution exists. It is crucial to think of vectorized solutions not iterative ones. I see this mistake all the time answering questions on stack overflow.\n",
    "\n",
    "## Chapter 5. Comparison to Pandas Cookbook\n",
    "DataFrame and Series construction is not covered as it is easily found in the documentation as mentioned previously. Pandas Cookbook spends 200 pages in chapters 1 through 6 to cover the fundamentals of the library using more interesting examples and real-world datasets. This includes sections:\n",
    "* Identifying each piece of the anatomy of DataFrames and Series\n",
    "* Understanding each data type and how to change them\n",
    "* Calling method in succession, called method chaining, which is mostly absent from PDA\n",
    "* Formalizing a routine whenever first importing a dataset\n",
    "* Selecting data in every idiomatic way possible\n",
    "* An entire chapter on boolean indexing\n",
    "* An entire chapter on index alignment, a very important topic that separates pandas from most other libraries\n",
    "* Much, much more in those first 6 chapters\n",
    "\n",
    "## Chapter 5. More Detailed Criticism\n",
    "\n",
    "One of the first code block mixes boolean indexing, vectorized multiplication and a numpy function being applied to a Series. There is too much functionality at once. I suppose this is covered in the NumPy chapter but I would break each of these operations out into their own sections, especially boolean indexing, which is quite a difficult concept for beginners to grasp.\n",
    "\n",
    "```\n",
    ">>> s = pd.Series(np.random.rand(3))\n",
    ">>> s[s > .5]\n",
    ">>> s * 2\n",
    ">>> np.sqrt(s)\n",
    "```\n",
    "\n",
    "PDA uses `’label’ in s` where s is a Series. This is not explicit. I think it would be better to do `’label’ in s.index` first and then explain how to use the implicit way.\n",
    "\n",
    "PDA uses a dictionary to create a Series with the additional parameter `index` passed a list where one of the values is not in the dictionary. Pandas actually allows this and creates a missing value. This is unnecessary complexity that is not needed at this point and is such a minute detail. It would be better reserved in a later chapter on more intricacies of constructor methods.\n",
    "\n",
    "```\n",
    "### Too complex and not used often for first pandas chapter\n",
    ">>> s = pd.Series({'a':1, 'b':10}, index=['a', 'b', 'c'])\n",
    ">>> s\n",
    "a     1.0\n",
    "b    10.0\n",
    "c     NaN\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "PDA uses the `isnull` and `notnull` functions to create a boolean Series and then shows how to do the same thing with the methods. I don't like introducing two operations to do the same thing. The more common way of doing this is with methods and not functions. The functions should be reserved for later chapters. I actually hardly ever use the `isnull/notnull` functions.\n",
    "\n",
    "```\n",
    "### Should use method and not function\n",
    ">>> s.isnull()\n",
    ">>> s.notnull()\n",
    "```\n",
    "\n",
    "PDA renames both the index level and the Series name with attributes. There is no context here why this is useful. I would save this for later when using `reset_index` which will use these attributes.\n",
    "\n",
    "```\n",
    ">>> s = pd.Series({'table':100, 'chair':40})\n",
    ">>> s\n",
    "chair     40\n",
    "table    100\n",
    "dtype: int64\n",
    "\n",
    "# Using reset_index pandas creates a DataFrame of two columns with default values for the column names\n",
    ">>> s.reset_index()\n",
    "```\n",
    "\n",
    "![](images/05_02_reset_index.png)\n",
    "\n",
    "``` \n",
    ">>> s.name='price'\n",
    ">>> s.index.name='item'\n",
    ">>> s.reset_index()\n",
    "```\n",
    "\n",
    "![](images/05_03_reset_index.png)\n",
    "\n",
    "\n",
    "PDA randomly uses the `loc` indexer with no explanation, towards the beginning of the chapter with `frame2`, which is likely to confuse the reader. It's not brought up again until several code blocks later. This should all be together.\n",
    "\n",
    "PDA uses the delete statement which isn't really ever used (I never use it). The `drop` method is idiomatic.\n",
    "\n",
    "PDA uses a nested dictionary of dictionaries to create a DataFrame. Again, it's not very useful to have this as one of your first interactions with pandas. It's rarely used.\n",
    "\n",
    "#### 5.2 Essential Functionality\n",
    "\n",
    "Ironically, the first thing under the section ‘Essential functionality’ is the `reindex` method which is only used twice in the entire book. If it’s so important, why is it used so infrequently? It is a useful method but it’s certainly not essential for those just being introduced to pandas.\n",
    "\n",
    "PDA uses the `drop` method with the `inplace` parameter which I never use, is not idiomatic, and which will be removed from future versions of pandas.\n",
    "\n",
    "There is even more selection with the indexing operator on a Series to  perform a slice. Again, this is not explicit. I always suggest using `loc` for labeled selection.\n",
    "\n",
    "PDA uses the indexing operator on a DataFrame. This is where lots of confusion is bound to happen. Passing a slice to the indexing operator is completely different than passing a scalar. The former selects rows and the latter, columns. PC has an entire recipe on this and then suggests not using slice notation for the indexing operator in a DataFrame as its primary function is to select columns and not rows. An example should clear this up:\n",
    "\n",
    "```\n",
    ">>> df = pd.DataFrame(np.random.rand(4, 3),\n",
    "                      columns=['Table', 'Chair', 'Bed'],\n",
    "                      index=['a', 'b', 'c', 'd'])\n",
    ">>> df\n",
    "```\n",
    "\n",
    "![](images/05_04_df.png)\n",
    "\n",
    "\n",
    "The primary purpose of the indexing operator is to select columns:\n",
    "```\n",
    ">>> df[['Table', 'Bed']]\n",
    "```\n",
    "\n",
    "![](images/05_04_dfslice.png)\n",
    "\n",
    "Confusingly, the indexing operator selects rows when given a slice\n",
    "\n",
    "```\n",
    ">>> df[2:]\n",
    "```\n",
    "\n",
    "![](images/05_05_df.png)\n",
    "\n",
    "PDA uses the `.loc` indexer by simultaneously selecting rows and columns. This is done by passing in row selection followed by a comma followed by column selection. I don’t like this introduction and prefer doing rows first followed by simultaneous selection. PC makes it very clear how the first element passed to `.loc` selects rows.\n",
    "\n",
    "PDA then uses some poor syntax with `df.iloc[:, :3][df.three > 5]`. It's bad practice and confusing to use indexing operators back to back like that. For beginners, I prefer doing this in two separate steps. I would then transition to showing a simultaneous boolean indexing with column selection in a single call to `.iloc` like this: `df.iloc[df.three.values > 5, :3]`. In fact, I would first do simultaneous boolean indexing and column selection with `.loc`. And, I would certainly never use the syntax from above.\n",
    "\n",
    "The section on 'Integer Indexes' is only going to confuse readers. This is why I suggest always using .loc/.iloc and only use the indexing operator for boolean indexing and DataFrame column selection. There are actually quite a few rules about how selection works with the indexing operator with several edge cases. I thought about diagraming this out in Pandas Cookbook, but thought better of it, as its very nuanced, hard to remember, and there are more preferable ways.\n",
    "\n",
    "The arithmetic methods with fill_values is fine but again has boring, fake data. There are lots more index alignment between two pandas objects, which is a fundamental part of pandas.\n",
    "\n",
    "PDA makes another particularly bad use of `apply` when it uses it to return the min and max of each column like this:\n",
    "\n",
    "```\n",
    ">>> df = pd.DataFrame(np.random.rand(2, 3), \n",
    "                      index=['a', 'b'], \n",
    "                      columns=['Table', 'Chair', 'Bed'])\n",
    ">>> def f(x):\n",
    "        return pd.Series([x.min(), x.max()], index=['min', 'max'])\n",
    "        \n",
    ">>> df.apply(f)\n",
    "```\n",
    "\n",
    "![](images/05_06_df.png)\n",
    "\n",
    "This would have been a great chance to use the newly added `agg` DataFrame method which returns the same exact DataFrame is a much simpler way:\n",
    "\n",
    "```\n",
    ">>> df.agg(['min', 'max'])\n",
    "```\n",
    "\n",
    "The newer versions of pandas-datareader have cleaner access to the main DataReader class. This is small but I prefer this:\n",
    "\n",
    "```\n",
    ">>> import pandas_datareader as pdr\n",
    ">>> pdr.DataReader('IBM', 'yahoo')\n",
    "```\n",
    "\n",
    "There is a small language error. `pd.value_counts` is a top-level function and not method. I also hardly ever use the function and would suggest people stick with methods, when they exist, as this is much more common.\n",
    "\n",
    "PDA uses the `apply` function with `pd.value_counts` in a very confusing manner. PDA should have definitely used strings to differentiate the counts. Here is the original, confusing DataFrame:\n",
    "\n",
    "```\n",
    ">>> df = pd.DataFrame({'a': [1, 3, 4, 3, 4],\n",
    "                       'b': [2, 3, 1, 2, 3],\n",
    "                       'c': [1, 5, 2, 4, 4]})\n",
    "# Very confusing here\n",
    ">>> df.apply(pd.value_counts)\n",
    "\n",
    "```\n",
    "\n",
    "![](images/05_07_vc.png)\n",
    "\n",
    "Just some better fake data would be better:\n",
    "\n",
    "```\n",
    ">>> df = pd.DataFrame({'a': ['table', 'chair', 'chair', 'lamp', 'bed'],\n",
    "                   'b': ['lamp', 'candle', 'chair', 'lamp', 'bed'],\n",
    "                   'c': ['mirror', 'mirror', 'mirror', 'mirror', 'mirror']})\n",
    "\n",
    ">>> df.apply(pd.value_counts).fillna(0)\n",
    "\n",
    "```\n",
    "\n",
    "![](images/05_08_vc2.png)\n",
    "\n",
    "There is actually a very interesting and advanced way of doing this combining `crosstab` and `melt`, which preserves data types\n",
    "\n",
    "```\n",
    ">>> pd.crosstab(**df.melt(var_name='columns', value_name='index'))\n",
    "```\n",
    "\n",
    "# Chapter 6. Data Loading, Storage, and File Formats\n",
    "### What it covers\n",
    "* Reading in a variety of txt files with `read_csv/read_table`\n",
    "* Setting index, data conversion, datetime parsing, skipping rows, set null values on read\n",
    "* Write to file or standard out with `to_csv`\n",
    "* iterating through files\n",
    "* reading in json and xml\n",
    "* reading/writing binary formats hdf5 and pickle\n",
    "* reading excel and connecting to sqlite\n",
    "\n",
    "## Chapter 6. Summary\n",
    "Chapter 6 covers many of the ways to read data into a pandas DataFrame from a file. There are many functions that begin with `read_` that can import nearly any kind of data format. Almost the entire chapter is covered in greater detail in the [IO Tools section of the documentation](http://pandas.pydata.org/pandas-docs/stable/io.html). There is no actual data analysis in this chapter, just the mechanics of reading and writing files.\n",
    "\n",
    "## Chapter 6. Criticisms\n",
    "PDA fails to point out that `read_csv` and `read_table` are the same exact function except that `read_csv` defaults to a comma-separated delimiter and `read_table` defaults to tab separated. There is absolutely no need for both of these functions. It's confusing to go back and forth between these two functions.\n",
    "\n",
    "There is a section on iterating through a file with the csv module. PDA shows the mechanics of this with a file that actually needs no further processing:\n",
    "\n",
    "```\n",
    ">>> pd.read_csv('examples/ex7.csv')\n",
    "```\n",
    "The data gets read just fine.\n",
    "\n",
    "![](images/06_01_csv.png)\n",
    "\n",
    "PDA then shows how to iterate through this file with the `csv` module. It could have been made much more interesting if there was some actual processing but there is none. It just shows how to iterate through the file. It also shows a confusing subclass of `csv.dialect` which is not needed. All the parameters can be passed directly into the `csv.reader` function.\n",
    "\n",
    "The `read_html` function uses nearly the [same dataset from the documentation](http://pandas.pydata.org/pandas-docs/stable/io.html#reading-html-content)\n",
    "\n",
    "There are very short sections on hdf5 and connecting to SQL which will not help you out much if you need to use them in a real situation. As usual, consult the documentation for full details.\n",
    "\n",
    "## Chapter 6. Comparison to Pandas Cookbook\n",
    "\n",
    "Chapter 6 is the only chapter that PDA has more coverage than PC. PC does read in lots of datasets but only covers a few of the options of the `read_` functions.\n",
    "\n",
    "PC has an enormous recipe with a fairly difficult to parse web page on Donald Trump’s vs Barack Obama’s approval rating. It uses several of the read_html parameters to read in just the right data. It then uses time series functionality and matplotlib to create plots.\n",
    "\n",
    "PC has a real 500k row dataset stored in HDF5 format that is used for many recipes. PC connects to a much more complex SQLite database and provides some info on the schema and makes some complex queries to it using showcasing the `merge` method.\n",
    "\n",
    "# Chapter 7. Data Cleaning and Preparation\n",
    "### What it covers\n",
    "* Finding, dropping and filling missing data with `isnull\\notnull`, `dropna`, and `fillna`\n",
    "* Finding duplicates with `drop_duplicates/duplicated`\n",
    "* Replacing values with `map` and `replace`\n",
    "* renaming index/column labels with `rename`\n",
    "* Binning data with `cut/qcut`\n",
    "* Encoding variables with `pd.get_dummies`\n",
    "* Basic string manipulation with Python strings, regex and pandas `str` accessor\n",
    "\n",
    "## Chapter 7. Summary:\n",
    "Chapter 7 follows the same pattern of using contrived and randomly generated data to show the basic mechanics of a few methods. It is devoid of actual data analysis and fails to show how to combine multiple pandas operations together. All of the methods in this chapter are covered in much greater detail in the documnetation. Visit the [Working with missing data section](http://pandas.pydata.org/pandas-docs/stable/missing_data.html) to get all the detailed mechanics of missing value manipulation in pandas. The chapter covers just a small amount of string manipulation. Visit the [Working with the Text Data section](http://pandas.pydata.org/pandas-docs/stable/text.html) to get more details.\n",
    "\n",
    "This chapter contains a large amount of poor and inefficient pandas code. There is an extreme example of looping to fill a DataFrame that can be accomplished in a much more concise code 100x faster.\n",
    "\n",
    "## Chapter 7.  Criticisms:\n",
    "\n",
    "The chapter begins by covering the `isnull` method, which is a little confusing because in Chapter 5, the function `pd.isnull` was used. I think a better job of distinguishing between methods and functions would be good here. In the 'Filtering out missing data' subsection, PDA write `pandas.isnull`. This is going to be confusing to beginners. Most people won't know if this a function or method. There is no consistency here. The most common way to get missing values is with the DataFrame/Series method and not the function.\n",
    "\n",
    "When the first DataFrame is constructed with random data, there is not even an effort to name the columns, so they simply get defaulted to integers which is confusing. \n",
    "\n",
    "PDA continually uses `axis=1` to refer to the columns. I highly advise against doing this as it is not as explicit as `axis='columns'`.\n",
    "\n",
    "#### Filling In Missing Data\n",
    "\n",
    "The integer columns become a problem when filling in missing data with a dictionary. It is rare to ever have integers as column names in real data and makes it so much more difficult to read. Take this: `df.fillna({1: 0.5, 2: 0})` which replaces the missing values in the column with the name `1` with .5 along and the missing values in the column with the name `2` with 0. \n",
    "\n",
    "Notice that the DataFrames are now named `df` which I suggested in chapter 5. This should be consistent throughout the book. Later in the chapter, both DataFrames and Series are named `data`. Just stick to `df` and `s`.\n",
    "\n",
    "#### Transforming Data Using a Function or Mapping\n",
    "\n",
    "As was mentioned earlier in the chapter 5 review, there should definitely be a distinction between `map` and `apply`. Ideally, `map` should only be used whenever you are passing a dictionary/Series to literally map one value to another. The `apply` method can only accept functions, so should always be used whenever you pass it a function. Yes, `map` has the same ability to accept functions in the same manner as `apply`, but this is why we need to have a separating boundary between the two methods to avoid confusion. In summary:\n",
    "\n",
    "* Use `map` only when you want to literally map each value in a Series to another. Your mapping must be a dictionary or a Series\n",
    "* Use `apply` when you have a function that you want to act on each individual member of the Series.\n",
    "* Never pass a function to `map`\n",
    "\n",
    "For instance, there is a Series with data like this:\n",
    "\n",
    "```\n",
    ">>> s = pd.Series(['Houston', 'Miami', 'Cleveland'])\n",
    ">>> states_map = {'houston':'Texas', 'miami':'Florida', 'Cleveland':'Ohio'}\n",
    "```\n",
    "\n",
    "PDA says to use a function with `map` like this:\n",
    "\n",
    "```\n",
    ">>> s.map(lambda x: states_map[x.lower()])\n",
    "0      Texas\n",
    "1    Florida\n",
    "2       Ohio\n",
    "dtype: object\n",
    "```\n",
    "\n",
    "But if you are going to use a function, you should use `apply` in the same exact manner. It gives the same result and it has more options than `map`.\n",
    "\n",
    "```\n",
    ">>> s.apply(lambda x: states_map[x.lower()])\n",
    "```\n",
    "\n",
    "There is actually a simpler and more idiomatic way. Generally, your mapping dictionary is going to be much smaller than the size of your Series. It makes more sense to conform the dictionary to the values in your Series. Here, we can simply change the keys in the dictionary (with the `title` string method) to match the values in our Series.\n",
    "\n",
    "Let's create a Series with 1 million elements and time the difference\n",
    "\n",
    "```\n",
    ">>> s1 = s.sample(1000000, replace=True)\n",
    "\n",
    ">>> %timeit s1.str.lower().map(states_map) # slow from PDA\n",
    "427 ms ± 12.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    ">>> %timeit s1.map({k.title(): v for k, v in states_map.items()})\n",
    "73.6 ms ± 909 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "\n",
    "```\n",
    "\n",
    "#### Renaming Axis Indexes\n",
    "There is some subtle trickiness with when running the command `df.rename(index=str.title, columns=str.upper)`. I like this method but there needs to be more explanation why this works. Most people are used to calling these as methods from an actual string but not through the constructor in this functional way.\n",
    "\n",
    "It also might be good to mention that you can use the `str` accessor directly on the index and columns like this:\n",
    "\n",
    "```\n",
    ">>> df.index = df.index.str.title()\n",
    ">>> df.columns = df.columns.str.upper()\n",
    "```\n",
    "\n",
    "#### Discretization and Binning\n",
    "\n",
    "The outcome of `pd.cut` is an ordered Categorical. You can see the less than signs between the categories. There should be at least one sentence explaining this.\n",
    "\n",
    "#### Detecting and Filtering Outliers\n",
    "\n",
    "There is some boolean indexing to find values in a Series that have absolute value greater than 3. It uses the NumPy function `abs`. NumPy functions should not be used when identical pandas methods are available:\n",
    "\n",
    "```\n",
    ">>> s = pd.Series(np.random.randn(1000))\n",
    ">>> s[np.abs(s) > 3]  # Don't use numpy\n",
    ">>> s[s.abs() > 3]    # use pandas methods\n",
    "\n",
    "```\n",
    "\n",
    "It's not a bad time to introduce inequality methods\n",
    "\n",
    "```\n",
    ">>> s[s.abs().gt(3)]\n",
    "```\n",
    "\n",
    "Chaining together methods makes it easier to see how the steps are progressing. Take for instance the next code block which does this:\n",
    "\n",
    "```\n",
    ">>> df = pd.DataFrame(np.random.randn(1000, 4))\n",
    ">>> df[(np.abs(df) > 3).any(1)]\n",
    "\n",
    "Simply and make verbose:\n",
    "\n",
    ">>> df[df.abs().gt(3).any('columns')]\n",
    "```\n",
    "\n",
    "A particularly bad piece of code happens when PDA caps the values in a DataFrame to be between -3 and 3 with this: `data[np.abs(data) > 3] = np.sign(data) * 3`. It is far easier to use the pandas clip method like this: `df.clip(-3, 3)`\n",
    "\n",
    "#### Permutation and Random Sampling\n",
    "The `take` method is used which is almost never used and I have never used it. The `.iloc` indexer was created to take integer values. The `take` method provides no additional functionality. This is one of the reasons pandas is confusing. There are multiple methods to do the same thing and `take` is unneeded now.\n",
    "\n",
    "```\n",
    ">>> sampler = [5, 2, 8, 10]\n",
    ">>> df.take(sampler) # outdated - dont use\n",
    ">>> df.iloc[sampler]  # idiomatic\n",
    "\n",
    "Using `np.sign` to convert DataFrame values t0 -1/1 is pretty swift.\n",
    "There is strange use of `np.sign` where a much easier pandas method `clip` exists\n",
    "```\n",
    "#### Computing Indicator/Dummy Variables\n",
    "This section has one of the worst code examples in the entire book. It does finally use real data. It reads in the data in a way that produces a warning. It should also use `read_csv` as again, it is the same exact function as `read_table`.\n",
    "\n",
    "```\n",
    ">>> mnames = ['movie_id', 'title', 'genres']\n",
    ">>> movies = pd.read_table('datasets/movielens/movies.dat', sep='::',\n",
    "                       header=None, names=mnames)\n",
    "                       \n",
    "# Produces a warning\n",
    "/Users/Ted/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
    "  This is separate from the ipykernel package so we can avoid doing imports until\n",
    "  \n",
    "# Shold be this\n",
    "\n",
    ">>> movies = pd.read_csv('datasets/movielens/movies.dat', sep='::', engine='python',\n",
    "                       header=None, names=mnames)\n",
    "```                       \n",
    "\n",
    "And I really dislike using the indexing operator to slice for rows. Use `head` instead.\n",
    "\n",
    "```\n",
    ">>> movies[:10] # ambiguous\n",
    ">>> movies.head(10) # use this instead\n",
    "```\n",
    "It continues iterating through the genres column which has strings of pipe separated values. It uses a combination of numpy, pandas and core python. This is a good opportunity to stay completely in pandas.\n",
    "\n",
    "```\n",
    ">>> movies.genres.str.split('|', expand=True).stack().unique()\n",
    "```\n",
    "\n",
    "This continues the use of the `str` accessor which is central to pandas string processing. PDA uses the builtin string methods unfortunately. It is true that the `stack` method has not been introduced but it works nicely here.\n",
    "\n",
    "To avoid `stack` you can do the following which is simpler than what is found in PDA:\n",
    "\n",
    "```\n",
    ">>> {g for genre in movies.genres.str.split('|') for g in set(genre)}\n",
    "```\n",
    "\n",
    "One of the most inefficient pieces of code in the entire books comes next where a DataFrame is constructed to show indicator columns (0/1) for each genre. The code creates an all zeros DataFrame then fills it with the rarely used `get_indexer` Index method.\n",
    "\n",
    "```\n",
    ">>> zero_matrix = np.zeros((len(movies), len(genres)))\n",
    ">>> dummies = pd.DataFrame(zero_matrix, columns=genres)\n",
    "\n",
    ">>> for i, gen in enumerate(movies.genres):\n",
    "        indices = dummies.columns.get_indexer(gen.split('|'))\n",
    "        dummies.iloc[i, indices] = 1\n",
    "\n",
    ">>> movies_windic = movies.join(dummies.add_prefix('Genre_'))\n",
    ">>> movies_windic.head()\n",
    "```\n",
    "\n",
    "![](images/07_01_movies.png)\n",
    "\n",
    "This code is incredibly slow and not idiomatic. The `movies` DataFrame is only 3,883 rows and takes a whopping **1.2** seconds to complete. There are several methods are much faster.\n",
    "\n",
    "An iterative approach would simply create a nested dictionary of dictionaries and pass this to the DataFrame constructor:\n",
    "\n",
    "```\n",
    ">>> d = {}\n",
    ">>> for key, values in movies.genres.items():\n",
    "        for v in values.split('|'):\n",
    "            if v in d:\n",
    "                d[v].update({key:1})\n",
    "            else:\n",
    "                d[v] = {key:1}       \n",
    ">>> df = movies.join(pd.DataFrame(d).fillna(0))\n",
    "```\n",
    "\n",
    "This takes only **14** ms about 85 times faster the approach used in PDA.\n",
    "\n",
    "Another method uses `pd.get_dummies` by first stacking all the genres into a single Series. This requires you to group by the original index and sum up all the rows:\n",
    "\n",
    "```\n",
    ">>> genres_stacked = movies.genres.str.split('|', expand=True).stack()\n",
    ">>> df_ind = pd.get_dummies(genres_stacked)\n",
    ">>> df_ind_total = df_ind.groupby(level=0).sum()\n",
    ">>> movies.join(df_ind_total)\n",
    "```\n",
    "\n",
    "This method actually has the benefit of keeping the data types as ints instead of floats. It also uses `pd.get_dummies` which was just introduced.\n",
    "\n",
    "#### Regular Expressions\n",
    "\n",
    "PDA fails to mention the term 'capture group' which is common terminology for using parentheses around parts of an expression.\n",
    "\n",
    "\n",
    "#### Vectorized String Functions in pandas\n",
    "\n",
    "This is a small section that won't help much. Again check the documentation for a huge number of examples\n",
    "\n",
    "## Chapter 7. Comparison to Pandas Cookbook\n",
    "There is no one chapter that focuses solely on data cleaning. Many of these methods are woven into several different recipes with real-life scenarios. There are a few interesting examples where binning continuous variables can make a huge difference during a data analysis such as when uncovering Simpson's paradox in the last recipe. The `get_dummies` method is quite useful but did not make it into PC.\n",
    "\n",
    "# Chapter 8. Data Wrangling: Join, Combine, and Reshape\n",
    "### What it covers\n",
    "* Creating a multiindex Series\n",
    "* Selection of a MultiIndex\n",
    "* Naming index levels\n",
    "* Swapping and sorting MultiIndexes with `swaplevel` and `sort_index`\n",
    "* Setting and resetting an index with `set_index` and `reset_index`\n",
    "* Combining DataFrames and Series with `merge`, `join`, `np.concatenate`, `pd.concat` and `combine_first`\n",
    "* Reshaping with `stack/unstack` and `pivot/melt`\n",
    "\n",
    "## Chapter 8. Summary:\n",
    "\n",
    "Chapter 8 is another chapter with fake data and covers very basic mechanics of many operations independently. There is very little code that combines multiple pandas methods together like is done in the real world. The hierarchical indexing is likely going to confuse readers as it is quite complex. I suggest reading the [documentation on hierarchical indexing](http://pandas.pydata.org/pandas-docs/stable/advanced.html). It will take awhile to grasp the mechanics of row and column selection.\n",
    "\n",
    "The sections on `merge` and `join` don't cover the differences between the two well enough and the examples again are very dry. For a more detailed summary of these methods, [see the docs](http://pandas.pydata.org/pandas-docs/stable/merging.html)\n",
    "\n",
    "## Chapter 8.  Criticisms:\n",
    "This chapter, again, begins by using the name `data` to refer to a Series. No consistency again. A Series with a MultiIndex is created and then different types of selection begin. Making a selection to a DataFrame and Series is actually very complex and not frequently done so easy to forget. \n",
    "\n",
    "In one block of code both the indexing operator and `.loc` are used to select Series elements. This is confusing and as I've said multiple times, stick with using only `.loc/.iloc` except when doing boolean indexing.\n",
    "\n",
    "It should be mentioned that passing a scalar element to the indexer does not return that level. For instance:\n",
    "\n",
    "```\n",
    ">>> s = pd.Series([10,3,2,11], index=[['a', 'a', 'b', 'b'],[1,2,3,1]])\n",
    ">>> s\n",
    "a  1    10\n",
    "   2     3\n",
    "b  3     2\n",
    "   1    11\n",
    "dtype: int64\n",
    "\n",
    ">>> s.loc['a'] # returns a single index Series\n",
    "1    10\n",
    "2     3\n",
    "dtype: int64\n",
    "\n",
    ">>> s.loc[['a']] # Preserves the MultiIndex\n",
    "a  1    10\n",
    "   2     3\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "It is also not mentioned that the outer level gets selected first.\n",
    "\n",
    "Perhaps, one of the most confusing selections is made next\n",
    "\n",
    "```\n",
    ">>> data.loc[:, 2]\n",
    "```\n",
    "\n",
    "Normally, a comma is used to separate row from column selections. Here it is selecting all of the outer level values that have the integer label 2 as the inner level. This gets more complicated when dealing with DataFames as this is just a Series. There should have been a section covering MultiIndex selection for DataFrames with several examples to clarify this.\n",
    "\n",
    "The axis levels are renamed by assigning the attributes directly. There is no mention of the newer `rename_axis`, which I use quite frequently.\n",
    "\n",
    "#### Reordering and Sorting Levels\n",
    "\n",
    "The `swaplevel` and `sort_index` levels are both passed the level numbers for most of their operations instead of the level names. I always use level names when available as its explicit and easy to forget the level number.\n",
    "\n",
    "#### Database-Style DataFrame Joins\n",
    "\n",
    "The first two DataFrames in this section have the `key` as the second column. Relational databases have this column first. PDA should have put this column first and any other non-key columns after.\n",
    "\n",
    "\n",
    "There are many pages that show the basic mechanics of merge, join, concat and combine_first with fake data.\n",
    "\n",
    "#### Pivoting “Long” to “Wide” Format\n",
    "Finally we get to a more interesting situation. There needs to be a mention of tidy vs messy data here from the Hadley Wickham . The current dataset is actually tidy and doesn't need any reshaping at all. And when the data is reshaped into 'long' data it actually becomes messy. The `reset_index` method has the `name` parameter to rename the Series values when it becomes a column. There is no need for a separate call to \n",
    "\n",
    "Also, there is an unnecessary call of the `rename` method.\n",
    "\n",
    "```\n",
    ">>> ldata = data.stack().reset_index().rename(columns={0: 'value'}) # unecessary\n",
    "\n",
    "### can shorten to:\n",
    ">>> ldata = data.stack().reset_index(name='value')\n",
    "```\n",
    "\n",
    "#### Pivoting “Wide” to “Long” Format\n",
    "In version 0.20 of pandas, the `melt` method was introduced. PDA uses the `melt` function which still works but going forward its preferable to use methods if they are available and do the same thing.\n",
    "\n",
    "## Chapter 8. Comparison to Pandas Cookbook\n",
    "PC creates numerous DataFrames that have MultiIndexes and shows how to rename levels with the `rename_axis` method. PC uses two chapters and over 100 pages to cover the material in this section. It covers a much greater variety of reshaping situations mainly dealing with Hadley Wickham's Tidy Data principles. All of the common types of messy datasets are reshaped into tidy data. \n",
    "\n",
    "The merge, join and concat methods/functions are covered in greater detail. The merge method is used in a more real setting by connecting to a relational database.\n",
    "\n",
    "# Chapter 9. Plotting and Visualization\n",
    "### What it covers\n",
    "* Introduction to matplotlib\n",
    "* Creating Axes and Figures\n",
    "* iterating through axes arrays and plotting\n",
    "* colors, linestyles, markers\n",
    "* ticks, labels, legends, text annotation\n",
    "* setting Axes properties\n",
    "* Adding patches\n",
    "* Very basic pandas and seaborn plotting\n",
    "\n",
    "## Chapter 9. Summary:\n",
    "Matplotlib is covered very lightly without the depth needed to understand the basics. Matplotlib has two distinct interfaces that users interact with to produces visualizations. This is the most fundamental piece of the library that needs to be understood. PDA uses both the stateful and object-oriented interfaces and sometimes in the same code-block. The matplotlib documentation specifically warns against doing this. Since this is just a small introduction to matplotlib, I think it would have been better to cover a single interface.\n",
    "\n",
    "The other concept to grasp the Figure-Axes hierarchy which is not explicitly mentioned. This is key to understanding how all plots work in matplotlib.\n",
    "\n",
    "The actual plotting is done mostly with random data and not in a real setting with multiple pandas operations taking place. It is a very mechanical chapter on how to use a few of the plotting features from matplotlib, pandas and seaborn.\n",
    "\n",
    "Matplotlib now has the ability to accept directly pandas DataFrames, which was left out of PDA.\n",
    "\n",
    "The other major issue with the chapter is the lack of comparison between pandas and seaborn plotting philosophies. Pandas uses wide or aggregated data for most of its plotting functions while seaborn needs tidy or long data.\n",
    "\n",
    "\n",
    "## Chapter 9.  Criticisms:\n",
    "\n",
    "It's a big mistake to not clarify the two different interfaces to make plots with matplotlib. It is only mentioned in passing that there are even two interfaces. You will have to attempt to infer from the code which one is which. Matplotlib is known for being a confusing library and this chapter perpetuates that sentiment.\n",
    "\n",
    "PDA creates a figure with the `figure` method and then goes on to add subplots one at a time with `add_subplot`. I really don't this introduction as there is no introduction to Figure - Axes hierachy. This hierarchy is crucial to understanding everything about matplotlib and its not even mentioned. \n",
    "\n",
    "There is a link to the matplotlib home page which won't help anyone find a particular item. PDA should have linked to specific parts of the documentation.\n",
    "\n",
    "The code block that adds patches, mixes the stateful and object-oriented interfaces. The matplotlib documentation says explicitly not to do this.\n",
    "\n",
    "It should be noted that matplotlib plotting functions can now take pandas DataFrames beginning from version 1.5. Not sure why this was not in PDA.\n",
    "\n",
    "When PDA moves to plotting with pandas, only a simple line plot and a few bar plots are created. There are several more plots that pandas is capable of creating - some with one variable and others with two variable. The stacked area plot is one of my favorites and isn't mentioned in PDA.\n",
    "\n",
    "All the pandas plotting uses the format `df.plot.bar` instead of `df.plot(kind='bar')`. They both produce the same thing, but the docstrings for the `df.plot.bar` is minimal which makes it much more difficult to remember the correct parameter names.\n",
    "\n",
    "PDA moves onto seaborn. There should be a very important distinction made between seaborn and pandas plotting. Seaborn uses long or tidy data, while pandas mainly uses wide or aggregated data. This is what makes seaborn so powerful. \n",
    "\n",
    "The data used for the pandas and seaborn plotting is the `tips` dataset that comes packaged with the seaborn library. There should have been more effort to find real datasets.\n",
    "\n",
    "Very little of the seaborn library was discussed and there was no mention of the difference between the seaborn functions that return an Axes vs those that return a seaborn Grid. This is crucial to understanding seaborn. The [seaborn docs](http://seaborn.pydata.org/tutorial.html) are much better than the little summary in PDA.\n",
    "\n",
    "## Chapter 9. Comparison to Pandas Cookbook\n",
    "PC covers matplotlib quite differently and first ensures that the reader is aware of the dual interface. It then proceeds to only cover the object-oriented interface as it is more Pythonic and gives more control. PC then covers the Figure - Axes hierarchy and makes sure that the reader understands that an Axes is matplotlib terminology for the plotting surface and not a plural for axis. \n",
    "\n",
    "PC covers the 'getter' and 'setter' methods that matplotlib uses to retrieve and change all the properties in the plot. The object-oriented interface is explicit and forces you to 'latch' onto objects in order to change their properties.\n",
    "\n",
    "PC focuses on combining multiple pandas operations together, and not just showing how independent commands function in isolation. For instance, PC uses historical movie budget data to calculate and then plot the rolling average over time. \n",
    "\n",
    "PC shows how pandas has been integrated into matplotlib by the addition of the `data` parameter. \n",
    "\n",
    "PC covers the difference between pandas and seaborn plotting clearly as mentioned above. PC uses boxplots to discover outlier flights that took much longer than expected to reach their destinatinon.\n",
    "\n",
    "One of my favorite recipes in PC is the discovery of emerging trends with data science meetup group data using a stacked area plot.\n",
    "\n",
    "PC has a recipe showing how pandas and seaborn create the same plot. This really makes it clear what the differences are.\n",
    "\n",
    "PC uses seaborn to graph up to 5 dimensions of data on the same Figure. This allows for an uncovering of Simpson's paradox, which is a commonly hidden in many datasets.\n",
    "\n",
    "# Chapter 10. Data Aggregation and Group Operations\n",
    "### What it covers\n",
    "* Grouping single/multiple columns and then aggregating single/multiple columns with single/multiple functions\n",
    "* Grouping by independent sequences not in DataFrame\n",
    "* Iterating over groups\n",
    "* Grouping with functions\n",
    "* Custom aggregating functions\n",
    "* Generic grouping with `apply`\n",
    "* Filling missing values in a group\n",
    "* Grouping with `axis=1`\n",
    "* Randomly sampling groups\n",
    "* Finding correlations within groups\n",
    "* Pivot tables and crosstabulation\n",
    "\n",
    "\n",
    "## Chapter 10. Summary:\n",
    "\n",
    "\n",
    "\n",
    "## Chapter 10.  Criticisms:\n",
    "\n",
    "The chapter begins with an image of the split-apply-combine process. In it, the 'key' column is on the left with the values on the right. In the first DataFrame created following this section the key columns come last. They should come first to match the image. Also, relational databases always place their key columns first before all other data.\n",
    "\n",
    "The groupby mechanics are introduced with the following syntax:\n",
    "\n",
    "```\n",
    ">>> grouped = df['data1'].groupby(df['key1'])\n",
    "```\n",
    "\n",
    "No one uses this syntax. I think the more common `df.groupby('key')['data']` should have been used first. This common syntax is covered a few code blocks later but it is confusing to cover  this unused syntax first.\n",
    "\n",
    "#### Iterating Over Groups\n",
    "PDA says that a useful recipe for getting groups is the following:\n",
    "\n",
    "```\n",
    ">>> pieces = dict(list(df.groupby('key1')))\n",
    ">>> pieces['b']\n",
    "```\n",
    "\n",
    "This is doing too much work and there already exists the `get_group` method that will do this for you:\n",
    "\n",
    "```\n",
    ">>> g = df.groupby('key')\n",
    ">>> g.get_group('b')\n",
    "```\n",
    "\n",
    "The next example shows the rarely used groupby with `axis=1` and iterating through each group like this:\n",
    "\n",
    "```\n",
    ">>> grouped = df.groupby(df.dtypes, axis=1)\n",
    ">>> for dtype, group in grouped:\n",
    "        print(dtype)\n",
    "        print(group)\n",
    "```\n",
    "\n",
    "There is no need to use groupby in this example. Pandas now comes equipped with the `select_dtpyes` method which can do the same thing in a more explicit manner like this:\n",
    "\n",
    "```\n",
    "for dtype in df.dtypes.unique():\n",
    "    print(dtype)\n",
    "    print(df.select_dtypes(dtype))\n",
    "```    \n",
    "\n",
    "####  Data Aggregation\n",
    "There needs to be a clearer definition for the term 'aggregation'. PDA says an aggregations 'produces scalar values'. To be very precise, it should say, 'produces a single scalar value'.\n",
    "\n",
    "PDA uses a list of two-item tuples for renaming columns within the `agg` groupby method. This is an undocumented feature and might get eliminated. Pandas recently deprecated the renaming of columns with a nested dictionary of dictionaries.\n",
    "\n",
    "#### Apply: General split-apply-combine\n",
    "The first example in this section uses the `apply` groupby method with a custom function to return the top tip percentages for each smoking/non-smoking group like this:\n",
    "\n",
    "```\n",
    ">>> def top(df, n=5, column='tip_pct'):\n",
    "        return df.sort_values(by=column)[-n:]\n",
    "        \n",
    ">>> tips.groupby('smoker').apply(top)\n",
    "\n",
    "```\n",
    "\n",
    "There actually is no need to use `apply` as its possible to sort the entire DataFrame first and then group and take the top rows like this:\n",
    "\n",
    "```\n",
    ">>> tips.sort_values('tip_pct', ascending=False) \\\n",
    "        .groupby('smoker').head()\n",
    "\n",
    "```\n",
    "\n",
    "Or using the `nth` method like this:\n",
    "\n",
    "```\n",
    ">>> tips.sort_values('tip_pct', ascending=False) \\\n",
    "        .groupby('smoker').nth(list(range(5)))\n",
    "```\n",
    "\n",
    "The custom function should use `iloc` or `tail` to be explicit and not the indexing operator to select the last n rows.\n",
    "\n",
    "In the next code block, extra arguments are passed to the custom function without any explanation of how it actually happens. There should be a discussion of `**kwargs` and `**args` which are part of the method signature.\n",
    "\n",
    "\n",
    "#### Quantile and Bucket Analysis\n",
    "\n",
    "There is some code that applies a custom to function to each group like this:\n",
    "```\n",
    ">>> def get_stats(group):\n",
    "        return {'min': group.min(), 'max': group.max(),\n",
    "                'count': group.count(), 'mean': group.mean()}\n",
    ">>> grouped = frame.data2.groupby(quartiles)\n",
    ">>> grouped.apply(get_stats).unstack()\n",
    "```\n",
    "\n",
    "There are a couple things here that would improve this. First, the most common syntax for the first line puts the column after the call to groupby. It makes it easier to read.\n",
    "\n",
    "```\n",
    ">>> frame.groupby(quartiles)['data2']\n",
    "```\n",
    "\n",
    "Second, PDA doesn't explain that returning a dictionary from `apply` on a groupby Series creates a MultiIndex. Chaining `unstack` here is just confusing. I would break this up into two steps.\n",
    "\n",
    "#### Example: Group-Wise Linear Regression\n",
    "\n",
    "The following function which computes simple linear regression: \n",
    "\n",
    "```\n",
    ">>> def regress(data, yvar, xvars):\n",
    "        Y = data[yvar]\n",
    "        X = data[xvars]\n",
    "        X['intercept'] = 1.\n",
    "        result = sm.OLS(Y, X).fit()\n",
    "        return result.params\n",
    "        \n",
    ">>> by_year.apply(regress, 'AAPL', ['SPX'])\n",
    "```\n",
    "\n",
    "Statsmodels has a builtin function to add the intercept so we can simplify it to:\n",
    "\n",
    "```\n",
    ">>> def regress(data, yvar, xvars):\n",
    "        Y = data[yvar]\n",
    "        X = sm.add_constant(data[xvars])\n",
    "        result = sm.OLS(Y, X).fit()\n",
    "        return result.params\n",
    "        \n",
    ">>> by_year.apply(regress, 'AAPL', 'SPX')\n",
    "```\n",
    "\n",
    "# Chapter 11. Time Series\n",
    "\n",
    "### What it covers\n",
    "* A few basics from the `datetime` standard library \n",
    "* Converting strings to timestamps with `pd.to_datetime`\n",
    "* Using string to slice Series/DataFrames with DatetimeIndex\n",
    "* Generating date ranges with `pd.date_range`\n",
    "* Date/time offsets with the `pandas.tseries.offsets` module\n",
    "* Shifting a DatetimeIndex an amout of time with `shift` with `freq` parameter\n",
    "* Time zone explanation, creation and conversion and the `tz` parameter, `tz_convert/tz_localize` methods\n",
    "* Period creation with `pd.Period/PeriodIndex`, ranges with `pd.period_range`\n",
    "* Converting from/to Periods/Timestamps with `to_period/to_timestamp`\n",
    "* Upsampling/downsampling with `resample`\n",
    "* Moving window functionality with `rolling/expanding/ewm` \n",
    "\n",
    "\n",
    "\n",
    "## Chapter 11. Summary:\n",
    "\n",
    "Chapter 11 on time series, is one of the driest and mechanical chapters of the entire book. Nearly all commands are executed with random or contrived data independently from other pandas commands. The [actual pandas time-series documentation](http://pandas.pydata.org/pandas-docs/stable/timeseries.html) covers this material in more detail.\n",
    "\n",
    "This chapter was written at the latest during pandas 0.18 as it does not show the `on` parameter for the `resample` method in table 11.5. This parameter makes it possible to use `resample` on any column not just the one in the index. The parameter was added in version 0.19 and by the time of the book release pandas was on 0.20. Also missing is the `tshift` method, which should definitely been mentioned as a direct replacement for the `shift` method when shifting the index by a time period.\n",
    "\n",
    "## Chapter 11.  Criticisms:\n",
    "\n",
    "`to_datetime` is referred to as a method when it's a function. \n",
    "\n",
    "The `pd.isnull` function is used when the more idiomatic `isnull` method should be used. \n",
    "\n",
    "There should be mention of the `tshift` method which shortens the syntax for time-shifting of an index.\n",
    "\n",
    "```\n",
    ">>> s.shift(5, freq='D')\n",
    "\n",
    "## becomes\n",
    "\n",
    ">>> s.tshift('5D')\n",
    "```\n",
    "\n",
    "In the \"Quarterly Period Frequencies\" there is use of the aliases 'e' and 's' to refer to start and end without explanation.\n",
    "\n",
    "You can tell that this book was written during Pandas version 0.18 as the `resample` method picked up a nice addition with the `on` parameter. This parameter is not seen in table 11-5 which shows the `resample` parameters.\n",
    "\n",
    "# Chapter 12. Advanced Pandas\n",
    "\n",
    "### What it covers\n",
    "* Creating categorical type with `astype('category')`\n",
    "* Creating categorical type with the constructor `pd.Categorical`\n",
    "* Categorical methods with the `cat` accessor\n",
    "* The `transform` groupby method\n",
    "* Grouping by time and another variable with the deprecated `pd.TimeGrouper`\n",
    "* Method chaining with `assign`, `pipe` and functions inside the inder `[ ]`\n",
    "\n",
    "\n",
    "## Chapter 12. Summary:\n",
    "\n",
    "This is one of the shortest chapters in the book and claims to have 'advanced' pandas operations. Most of the operations covered here are fundamental to the library. The section on method chaining does have a few more complex examples. \n",
    "\n",
    "One major flaw of this chapter is the use of the deprecated `pd.TimeGrouper` instead of `pd.Grouper`. PDA says that one of the limitations of `pd.TimeGrouper` is that it can only group times in the index. `pd.Grouper` has been able to do time grouping on columns other than the index for a long time. Additionally, it is possible to chain the `resample` method following the `groupby` method to do group by both time and any other groups of columns.\n",
    "\n",
    "\n",
    "## Chapter 12.  Criticisms:\n",
    "\n",
    "The `take` method, which preceded the development of the `.iloc` method is used here. No one uses this method and it should definitely be replaced with `.iloc`. Looking at stackoverflow's search feature, `df.take` returns [12 results](https://stackoverflow.com/search?q=%5Bpandas%5D+df.take) while `df.iloc` returns [over 2100 results](https://stackoverflow.com/search?q=%5Bpandas%5D+df.iloc).\n",
    "\n",
    "This line really shows how PDA was not updated correctly. The iloc indexer is very important and is not used nearly as much as it should have been.\n",
    "\n",
    "During the creation of the ordered categorical sequence, the `sort_values` method should have been used to show how sorting can be based on the categories numbers and not the string values themselves:\n",
    "\n",
    "\n",
    "```\n",
    ">>> categories = ['foo', 'bar', 'baz']\n",
    "\n",
    ">>> codes = [0, 1, 2, 0, 0, 1]\n",
    "\n",
    ">>> ordered_cat = pd.Categorical.from_codes(codes, categories, ordered=True)\n",
    "\n",
    "### show original order\n",
    ">>> ordered_cat\n",
    "[foo, bar, baz, foo, foo, bar]\n",
    "Categories (3, object): [foo < bar < baz]\n",
    "\n",
    ">>> ordered_cat.sort_values()\n",
    "[foo, foo, foo, bar, bar, baz]\n",
    "Categories (3, object): [foo < bar < baz]\n",
    "```\n",
    "\n",
    "#### BETTER PERFORMANCE WITH CATEGORICALS\n",
    "\n",
    "I think there was some code, perhaps accidentally, left out of this section. PDA mentions the performance increase with using categoricals but does not actually compute any performance except for the creation of the categorical. I think PDA meant to do a simple calculation like `value_counts` on each of the Series like this:\n",
    "\n",
    "```\n",
    ">>> N = 10000000\n",
    ">>> draws = pd.Series(np.random.randn(N))\n",
    ">>> labels = pd.Series(['foo', 'bar', 'baz', 'qux'] * (N // 4))\n",
    ">>> categories = labels.astype('category')\n",
    "\n",
    ">>> %timeit labels.value_counts()\n",
    "1.14 s ± 20.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n",
    ">>> %timeit categories.value_counts()\n",
    "41.7 ms ± 746 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
    "```\n",
    "\n",
    "#### CREATING DUMMY VARIABLES FOR MODELING\n",
    "The example used in this section could have been much more powerful if it used integers instead of strings. Strings automatically get encoded with `pd.get_dummies`.  For instance, there isn't a need to force the column of strings to category:\n",
    "```\n",
    ">>> cat_s = pd.Series(['a', 'b', 'c', 'd'] * 2, dtype='category')\n",
    ">>> s = pd.Series(['a', 'b', 'c', 'd'] * 2)\n",
    "\n",
    ">>> d1 = pd.get_dummies(cat_s)\n",
    ">>> d2 = pd.get_dummies(s)\n",
    ">>> d1.equals(d2)\n",
    "True\n",
    "```\n",
    "\n",
    "A better example would have used integers. `pd.get_dummies` ignores numerical columns but does make a new column for each categorical level no matter what its underlying data type is:\n",
    "\n",
    "```\n",
    ">>> s = pd.Series([50, 10, 8, 10, 50] , dtype='category')\n",
    ">>> pd.get_dummies(s)\n",
    "\n",
    "```\n",
    "\n",
    "#### Grouped Time Resampling\n",
    "This section has several problems. PDA promises to be updated to Pandas version 0.20 but it is not. The `resample` method, since version 0.19, has the `on` parameter. Let's see how PDA does the following operation:\n",
    "\n",
    "```\n",
    ">>> times = pd.date_range('2017-05-20 00:00', freq='1min', periods=N)\n",
    ">>> df = pd.DataFrame({'time': times, 'value': np.arange(N)})\n",
    "\n",
    ">>> df.set_index('time').resample('5min').count()\n",
    "```\n",
    "\n",
    "Using the latest versions of pandas, you can do this:\n",
    "\n",
    "```\n",
    ">>> df.resample('5min', on='time).count()\n",
    "```\n",
    "\n",
    "![](images/12_01_gd.png)\n",
    "\n",
    "The next problem arises when grouping by both time and another column. PDA uses the deprecated `pd.TimeGrouper`, which has never had any documentation. PDA does the following:\n",
    "\n",
    "```\n",
    ">>> df2 = pd.DataFrame({'time': times.repeat(3),\n",
    "                    'key': np.tile(['a', 'b', 'c'], N),\n",
    "                    'value': np.arange(N * 3.)})\n",
    "                    \n",
    ">>> time_key = pd.TimeGrouper('5min')  # deprecated so never use\n",
    "\n",
    ">>> resampled = (df2.set_index('time')\n",
    "                    .groupby(['key', time_key])\n",
    "                    .sum())\n",
    "```\n",
    "To do this using modern pandas, do the following:\n",
    "\n",
    "```\n",
    ">>> df2.groupby(['key', pd.Grouper(key='time', freq='5T')]).sum()\n",
    "```\n",
    "\n",
    "The groupby object actually has a `resample` method:\n",
    "\n",
    "```\n",
    ">>> df2.groupby('key').resample('5T', on='time').sum()\n",
    "```\n",
    "#### Techniques for Method Chaining\n",
    "This section uses no actual data, so you can't actually run the code. \n",
    "\n",
    "Since this section is on method chaining it would make sense to use the `sub` method. The following code appears in PDA:\n",
    "\n",
    "```\n",
    ">>> df2.assign(col1_demeaned=df2.col1 - df2.col2.mean())\n",
    "```\n",
    "\n",
    "This can be replaced with:\n",
    "```\n",
    ">>> df2.assign(col1_demeaned=df2.col1.sub(df2.col2.mean()))\n",
    "```\n",
    "\n",
    "The next code block uses the \"selection by callable\" which you can [see here in the documentation](http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-callable). I am a little shocked that the `query` method is not mentioned in the entire book which can convert this ugly code that appears in PDA:\n",
    "\n",
    "```\n",
    ">>> df = (load_data()\n",
    "      [lambda x: x['col2'] < 0])\n",
    "```\n",
    "\n",
    "To this:\n",
    "\n",
    "```\n",
    ">>> df = load_data().query('col2 < 0')\n",
    "```\n",
    "\n",
    "# Chapter 13. Introduction to Modeling Libraries in Python\n",
    "### What it covers\n",
    "* \n",
    "\n",
    "\n",
    "## Chapter 13. Summary:\n",
    "\n",
    "\n",
    "\n",
    "## Chapter 13.  Criticisms:\n",
    "\n",
    "\n",
    "The following section of code is from PDA and creates some fake data and a categorical column:\n",
    "\n",
    "```\n",
    ">>> data = pd.DataFrame({'x0': [1, 2, 3, 4, 5],\n",
    "                         'x1': [0.01, -0.01, 0.25, -4.1, 0.],\n",
    "                         'y': [-1.5, 0., 3.6, 1.3, -2.]})\n",
    "                         \n",
    ">>> data['category'] = pd.Categorical(['a', 'b', 'a', 'a', 'b'],\n",
    "                                   categories=['a', 'b'])\n",
    "```\n",
    "\n",
    "This categorical column is then encoded as two separate columns, one for each category.\n",
    "\n",
    "```\n",
    ">>> dummies = pd.get_dummies(data.category, prefix='category')\n",
    ">>> data_with_dummies = data.drop('category', axis=1).join(dummies)\n",
    "```\n",
    "\n",
    "There is a much easier way. You can just drop the whole DataFrame into `pd.get_dummies` and it will ignore the numeric columns while encoding all the object/category columns like this:\n",
    "\n",
    "```\n",
    ">>> pd.get_dummies(data)\n",
    "```\n",
    "\n",
    "There's actually a pretty cool application of `assign`, starred args and `get_dummies` that does this whole operation from data creation to final product in one step:\n",
    "\n",
    "```\n",
    ">>> data = pd.DataFrame({\n",
    "   ....:     'x0': [1, 2, 3, 4, 5],\n",
    "   ....:     'x1': [0.01, -0.01, 0.25, -4.1, 0.],\n",
    "   ....:     'y': [-1.5, 0., 3.6, 1.3, -2.]})\n",
    "\n",
    "data.assign(**pd.get_dummies(['a', 'b', 'a', 'a', 'b']))\n",
    "```\n",
    "\n",
    "The rest of the chapter covers patsy, statsmodels and scikit-learn libraries briefly. Patsy is not a commonly used library, so I'm not sure why it's even in the book. It's had no commits to it since February of 2017 and only a total of 46 total questions on stackoverflow. Compare this to the 50,000+ questions on pandas. \n",
    "\n",
    "# Chapter 14. Data Analysis Examples\n",
    "\n",
    "### What it covers\n",
    "* Reading in json with the json Python library\n",
    "*\n",
    "\n",
    "\n",
    "## Chapter 14. Summary:\n",
    "\n",
    "\n",
    "\n",
    "## Chapter 14.  Criticisms:\n",
    "\n",
    "#### USA.gov Data from Bitly\n",
    "The `pd.read_json` function can and should be used to read in the data from bitly. This function has a new parameter, `lines`, that when set to `True` can read in multiple lines of json data directly into a DataFrame. This is rather large omission:\n",
    "\n",
    "```\n",
    ">>> frame = pd.read_json('datasets/bitly_usagov/example.txt', lines=True) \n",
    "```\n",
    "\n",
    "After counting the time zones, PDA uses seaborn to do a barplot, which is fine but it is also one of the few instances where pandas can do it easier:\n",
    "\n",
    "```\n",
    ">>> frame.tz.value_counts().iloc[:10].plot(kind='barh')\n",
    "```\n",
    "\n",
    "There is the seaborn `countplot` that does do the counting like this with the need for `value_counts`, but it doesn't order and it doesn't sort:\n",
    "\n",
    "```\n",
    ">>> sns.countplot('tz', data=frame)\n",
    "```\n",
    "\n",
    "The field names need to be renamed for better comprehension. Having the column `a` refer to the internet browsers makes no sense. All the other columns are uninterpretable as well.\n",
    "\n",
    "The code that counts the browsers is poor and can be significantly improved. Here is the original:\n",
    "\n",
    "```\n",
    ">>> results = pd.Series([x.split()[0] for x in frame.a.dropna()])\n",
    ">>> results.value_counts()[:8]\n",
    "```\n",
    "\n",
    "It can be modernized to the following which uses the `str` accessor:\n",
    "\n",
    "```\n",
    ">>> frame['a'].str.split().str[0].value_counts().head(8)\n",
    "```\n",
    "\n",
    "There is a cross-tabulation between OS and timezone. It is done in the following manner:\n",
    "\n",
    "```\n",
    ">>> by_tz_os = cframe.groupby(['tz', 'os'])\n",
    ">>> agg_counts = by_tz_os.size().unstack().fillna(0)\n",
    "```\n",
    "\n",
    "There is no need to chain fillna here as the `unstack` method has its own `fill_value` parameter:\n",
    "\n",
    "```\n",
    ">>> agg_counts = cframe.groupby(['tz', 'os']).size().unstack(fill_value=0)\n",
    "```\n",
    "\n",
    "You can also use the `crosstab` function directly:\n",
    "\n",
    "```\n",
    ">>> agg_counts = pd.crosstab(cframe['tz'], cframe['os'])\n",
    "```\n",
    "\n",
    "The following code block finds the top 10 time zones by row total using the old `take` method:\n",
    "\n",
    "```\n",
    ">>> indexer = agg_counts.sum(1).argsort()\n",
    ">>> count_subset = agg_counts.take(indexer[-10:])\n",
    "```\n",
    "\n",
    "A more modern approach is with `.loc`:\n",
    "\n",
    "```\n",
    ">>> idx = agg_counts.sum(axis='columns').sort_values().index[-10:]\n",
    ">>> count_subset = agg_counts.loc[idx]\n",
    "```\n",
    "\n",
    "You can actually use the `crosstab` function with the `margins` parameter to do all these steps in one line\n",
    "\n",
    "```\n",
    ">>> pd.crosstab(frame['tz'], frame['os'], margins=True).sort_values('All', ascending=False)\n",
    "```\n",
    "\n",
    "PDA uses seaborn to create a horizontal barplot. This is not an ideal usecase for  seaborn as the data is already aggregated. Seaborn works best with tidy un-aggregated data. Pandas is the correct tool for this. PDA uses 4 lines of code for this:\n",
    "\n",
    "```\n",
    ">>> count_subset = count_subset.stack()\n",
    ">>> count_subset.name = 'total'\n",
    ">>> count_subset = count_subset.reset_index()\n",
    ">>> sns.barplot(x='total', y='tz', hue='os',  data=count_subset)\n",
    "```\n",
    "\n",
    "When only one line of code is needed for pandas\n",
    "\n",
    "```\n",
    ">>> count_subset.plot(kind='barh')\n",
    "```\n",
    "\n",
    "The next code block computes the percentage of windows/non-windows users by doing another groupby like this:\n",
    "\n",
    "```\n",
    ">>> def norm_total(group):\n",
    "        group['normed_total'] = group.total / group.total.sum()\n",
    "        return group\n",
    "\n",
    ">>> results = count_subset.groupby('tz').apply(norm_total)\n",
    "```\n",
    "\n",
    "There is no need to do another groupby. In fact this whole exercise would have been much easier if the top 10 timezones were first stored into a variable.\n",
    "\n",
    "```\n",
    ">>> idx_top10 = frame['tz'].value_counts().index[:10]\n",
    "```\n",
    "\n",
    "The percentages could have been retrieved from pandas `crosstab` function with `normalize=True`:\n",
    "\n",
    "```\n",
    ">>> pd.crosstab(frame['tz'], frame['os'], normalize=True).loc[idx_top10]\n",
    "```\n",
    "\n",
    "#### MovieLens 1M Dataset\n",
    "\n",
    "When reading in the data I strongly suggest to only use `read_csv` as there is no difference between it and  `read_table` other than the default delimiter. Also, there is a warning that is raised which can be eliminated with specifying `engine='python'`.\n",
    "\n",
    "The top 5 rows of the table are displayed with `[:5]`, which I really dislike. This should be done using the `head` method.\n",
    "\n",
    "PDA joins all three tables together like this:\n",
    "\n",
    "```\n",
    ">>> pd.merge(pd.merge(ratings, users), movies)\n",
    "```\n",
    "\n",
    "I prefer to use methods and be explicit with the joining columns:\n",
    "\n",
    "```\n",
    ">>> users.merge(ratings, on='user_id').merge(movies, on='movie_id')\n",
    "```\n",
    "\n",
    "\n",
    "The movies with at least 250 ratings are found in the next block of code like this:\n",
    "\n",
    "```\n",
    ">>> ratings_by_title = data.groupby('title').size()\n",
    ">>> active_titles = ratings_by_title.index[ratings_by_title >= 250]\n",
    "```\n",
    "\n",
    "This would have been a good opportunity to use a 'callable by selection' after calling `value_counts`. There is no need for a `groupby` here. So, we can modify the above to this:\n",
    "\n",
    "```\n",
    ">>> active_titles = data['title'].value_counts()[lambda x: x >= 250].index\n",
    "```\n",
    "\n",
    "It would have also been an opportunity to use the `filter` groupby method which was not used at all during the entire book:\n",
    "\n",
    "```\n",
    ">>> data.groupby('movie_id')['title'].filter(lambda x: len(x) >= 250).drop_duplicates().values\n",
    "```\n",
    "\n",
    "Throughout this whole analysis and the whole book there is a noticeable absence of the `.iloc` indexing operator. I always use it and never use the indexing operator by itself to select rows with a slice.\n",
    "\n",
    "#### US Baby Names 1880–2010\n",
    "\n",
    "It would have been nice to see updated data to include 2016. There's even a link in the book to source of the data. It would have been so easy to update the data and should have been done. It shows the little amount of effort taken to update the book.\n",
    "\n",
    "The book says it is updated to 3.6 so it might be good to show fstrings when doing string interpolation. Or to show the `format` method like this:\n",
    "\n",
    "```\n",
    "path = 'datasets/babynames/yob{}.txt'.format(year)\n",
    "```\n",
    "\n",
    "After all the baby names are concatenated into a single dataframe, PDA adds a column for the proportion for each name of each sex like this:\n",
    "\n",
    "```\n",
    ">>> def add_prop(group):\n",
    "        group['prop'] = group.births / group.births.sum()\n",
    "        return group\n",
    "    \n",
    ">>> names = names.groupby(['year', 'sex']).apply(add_prop)\n",
    "```\n",
    "\n",
    "Notice that in the `add_prop` function, the passed sub-dataframe is being modified and returned. There is no need to return an entire DataFrame here. Instead, you can change use the `transform` method on only the `births` column:\n",
    "\n",
    "```\n",
    "names['prop'] = names.groupby(['year', 'sex'])['births'].transform(lambda x: x / x.sum())\n",
    "```\n",
    "\n",
    "This is much more straightforward and about twice as fast.\n",
    "\n",
    "PDA then offers two ways to get the top 1,000 names for each sex of each year. One way like this:\n",
    "\n",
    "```\n",
    ">>> def get_top1000(group):\n",
    "        return group.sort_values(by='births', ascending=False)[:1000]\n",
    ">>> grouped = names.groupby(['year', 'sex'])\n",
    ">>> top1000 = grouped.apply(get_top1000)\n",
    ">>> top1000.reset_index(inplace=True, drop=True)\n",
    "```\n",
    "\n",
    "Its possible to pre-sort the data and then use the `head` groupby method. This method also doesn't need any resetting or dropping of the index.\n",
    "\n",
    "```\n",
    ">>> names_sorted = names.sort_values(['year', 'sex', 'births'], ascending=[True, True, False])\n",
    ">>> top1000 = names_sorted.groupby(['year', 'sex']).head(1000)\n",
    "```\n",
    "\n",
    "In the next code block, PDA gets the number of names it takes to make up 50% of the distribution for each year-sex grouping:\n",
    "\n",
    "```\n",
    "def get_quantile_count(group, q=0.5):\n",
    "    group = group.sort_values(by='prop', ascending=False)\n",
    "    return group.prop.cumsum().values.searchsorted(q) + 1\n",
    "\n",
    "diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)\n",
    "```\n",
    "\n",
    "There is no need to use `searchsorted` here. A simpler comparison against .5 would work the same and is faster.\n",
    "\n",
    "```\n",
    ">>> top1000.groupby(['year', 'sex']).apply(lambda x: (x.prop.cumsum() < .5).sum() + 1)\n",
    "```\n",
    "\n",
    "PDA extracts the last letter of each baby name like this:\n",
    "\n",
    "```\n",
    ">>> get_last_letter = lambda x: x[-1]\n",
    ">>> last_letters = names.name.map(get_last_letter)\n",
    "```\n",
    "\n",
    "This is confusing as I have mentioned previously, `apply` and `map` do the same thing when passed a function. I always use `apply` here to differentiate the two.\n",
    "\n",
    "Pandas has built-in functionality to get any character from a column of strings like this:\n",
    "\n",
    "```\n",
    ">>> names.name.str[-1]\n",
    "```\n",
    "\n",
    "Several steps follow to get the percentage of last letters for year and sex.\n",
    "\n",
    "```\n",
    ">>> table = names.pivot_table('births', index=last_letters,\n",
    "                          columns=['sex', 'year'], aggfunc=sum)\n",
    "                          \n",
    ">>> subtable = table.reindex(columns=[1910, 1960, 2010], level='year')\n",
    "\n",
    ">>> letter_prop = subtable / subtable.sum()\n",
    "\n",
    "```\n",
    "\n",
    "This can all be done much quicker by first filtering the data to just the years 1910, 1960, 2010 and then using `pd.crosstab`:\n",
    "\n",
    "```\n",
    ">>> names2 = names.query('year in [1910, 1960, 2010]')\n",
    "\n",
    ">>> letter_prop = pd.crosstab(index=names2.name.str[-1], rownames=['last letter'],\n",
    "                              columns=[names2.sex, names2.year], normalize='columns')\n",
    "```\n",
    "\n",
    "The plot of these letter proportions are plotted next with pandas. But, this would have been an excellent opportunity to use seaborn. There would be no need for any of this calculation of the `letter_prop` table. Seaborn does all this calculation under the hood.\n",
    "\n",
    "```\n",
    ">>> sns.factorplot(x='last_letter', y='prop', hue='year', \n",
    "                   data=names2.sort_values('last_letter'), \n",
    "                   kind='bar', row='sex', estimator=np.sum, ci=0)\n",
    "```\n",
    "\n",
    "The most interesting analysis in the whole book comes with the name Leslie name change from male to female. There are many more fun things to discover. Five thirty eight ran a [fun story on names](https://fivethirtyeight.com/features/how-to-tell-someones-age-when-all-you-know-is-her-name/) a few years back with many more interesting charts. See this one for the name 'Brittany':\n",
    "\n",
    "![](images/14_brittany.png)\n",
    "\n",
    "#### USDA Food Database\n",
    "\n",
    "An entire chunk of code is missing from the book. It made it into the online Jupyter notebooks but I guess was forgot to be put into the actual book. This will be difficult for beginners to notice:\n",
    "\n",
    "```\n",
    "# Missing code from book\n",
    ">>> nutrients = []\n",
    "\n",
    ">>> for rec in db:\n",
    "        fnuts = pd.DataFrame(rec['nutrients'])\n",
    "        fnuts['id'] = rec['id']\n",
    "        nutrients.append(fnuts)\n",
    "\n",
    ">>> nutrients = pd.concat(nutrients, ignore_index=True)\n",
    "```\n",
    "\n",
    "PDA uses a convoluted way to find the food with the most nutrients:\n",
    "\n",
    "```\n",
    ">>> by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])\n",
    "\n",
    ">>> get_maximum = lambda x: x.loc[x.value.idxmax()]\n",
    "\n",
    ">>> max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]\n",
    "```\n",
    "\n",
    "A more clever approach involves sorting the column we want the maximum for (`value` in this case) and then dropping all rows other than the first in the group with `drop_duplicates`. You must use the `subset` parameter here:\n",
    "```\n",
    ">>> max_foods = ndata.sort_values('value', ascending=False).drop_duplicates(subset=['nutrient'])\n",
    "```\n",
    "\n",
    "#### 2012 Federal Election Commission Database\n",
    "This should have been 2016 data since the book was published a year after the 2016 election.\n",
    "\n",
    "The `unstack` method should be used with the `fill_value` parameter to fill in 0's wherever there is missing data. It does not and instead returns NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Code\n",
    "\n",
    "# Chapter 5 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(data=[9, -5, 13], index=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     9\n",
       "b    -5\n",
       "c    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images/05_04_dfslice.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loc['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Quebec</th>\n",
       "      <td>1.154130</td>\n",
       "      <td>0.301991</td>\n",
       "      <td>-0.759571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ontario</th>\n",
       "      <td>-0.425998</td>\n",
       "      <td>-0.270541</td>\n",
       "      <td>0.532976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alberta</th>\n",
       "      <td>-0.163457</td>\n",
       "      <td>-0.138279</td>\n",
       "      <td>1.279828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nova Scotia</th>\n",
       "      <td>-1.446225</td>\n",
       "      <td>-0.578367</td>\n",
       "      <td>-0.022417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    a         b         c\n",
       "Quebec       1.154130  0.301991 -0.759571\n",
       "Ontario     -0.425998 -0.270541  0.532976\n",
       "Alberta     -0.163457 -0.138279  1.279828\n",
       "Nova Scotia -1.446225 -0.578367 -0.022417"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randn(4, 3), columns=list('abc'),\n",
    "        index=['Quebec', 'Ontario', 'Alberta', 'Nova Scotia'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    2.600356\n",
       "b    0.880358\n",
       "c    2.039398\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(lambda x: x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    2.600356\n",
       "b    0.880358\n",
       "c    2.039398\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max() - df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     1.0\n",
       "b    10.0\n",
       "c     NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series({'a':1, 'b':10}, index=['a', 'b', 'c'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chair     40\n",
       "table    100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series({'table':100, 'chair':40})\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chair</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    0\n",
       "0  chair   40\n",
       "1  table  100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chair</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    item  price\n",
       "0  chair     40\n",
       "1  table    100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.name='price'\n",
    "s.index.name='item'\n",
    "s.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Chair</th>\n",
       "      <th>Bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.587026</td>\n",
       "      <td>0.622388</td>\n",
       "      <td>0.614905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.857654</td>\n",
       "      <td>0.876305</td>\n",
       "      <td>0.117314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.906308</td>\n",
       "      <td>0.865932</td>\n",
       "      <td>0.268340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.726397</td>\n",
       "      <td>0.720055</td>\n",
       "      <td>0.087585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Table     Chair       Bed\n",
       "a  0.587026  0.622388  0.614905\n",
       "b  0.857654  0.876305  0.117314\n",
       "c  0.906308  0.865932  0.268340\n",
       "d  0.726397  0.720055  0.087585"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.rand(4, 3),\n",
    "                      columns=['Table', 'Chair', 'Bed'],\n",
    "                      index=['a', 'b', 'c', 'd'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.672490</td>\n",
       "      <td>0.772336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.813925</td>\n",
       "      <td>0.095847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.473302</td>\n",
       "      <td>0.230279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.146679</td>\n",
       "      <td>0.346614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Table       Bed\n",
       "a  0.672490  0.772336\n",
       "b  0.813925  0.095847\n",
       "c  0.473302  0.230279\n",
       "d  0.146679  0.346614"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Table', 'Bed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Chair</th>\n",
       "      <th>Bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.906308</td>\n",
       "      <td>0.865932</td>\n",
       "      <td>0.268340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.726397</td>\n",
       "      <td>0.720055</td>\n",
       "      <td>0.087585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Table     Chair       Bed\n",
       "c  0.906308  0.865932  0.268340\n",
       "d  0.726397  0.720055  0.087585"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Chair</th>\n",
       "      <th>Bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.261308</td>\n",
       "      <td>0.621405</td>\n",
       "      <td>0.881686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.215955</td>\n",
       "      <td>0.305033</td>\n",
       "      <td>0.155414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.589533</td>\n",
       "      <td>0.263043</td>\n",
       "      <td>0.796692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.852403</td>\n",
       "      <td>0.790783</td>\n",
       "      <td>0.170018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.540780</td>\n",
       "      <td>0.574551</td>\n",
       "      <td>0.790692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.698829</td>\n",
       "      <td>0.572204</td>\n",
       "      <td>0.369239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.526655</td>\n",
       "      <td>0.606730</td>\n",
       "      <td>0.905878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.527964</td>\n",
       "      <td>0.097550</td>\n",
       "      <td>0.180797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Table     Chair       Bed\n",
       "0  0.261308  0.621405  0.881686\n",
       "1  0.215955  0.305033  0.155414\n",
       "2  0.589533  0.263043  0.796692\n",
       "3  0.852403  0.790783  0.170018\n",
       "4  0.540780  0.574551  0.790692\n",
       "5  0.698829  0.572204  0.369239\n",
       "6  0.526655  0.606730  0.905878\n",
       "7  0.527964  0.097550  0.180797"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.rand(8, 3), \n",
    "                      columns=['Table', 'Chair', 'Bed'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Chair</th>\n",
       "      <th>Bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.215955</td>\n",
       "      <td>0.097550</td>\n",
       "      <td>0.155414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.852403</td>\n",
       "      <td>0.790783</td>\n",
       "      <td>0.905878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Table     Chair       Bed\n",
       "min  0.215955  0.097550  0.155414\n",
       "max  0.852403  0.790783  0.905878"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return pd.Series([x.min(), x.max()], index=['min', 'max'])\n",
    "        \n",
    "df.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table</th>\n",
       "      <th>Chair</th>\n",
       "      <th>Bed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.028480</td>\n",
       "      <td>0.375552</td>\n",
       "      <td>0.403832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.739077</td>\n",
       "      <td>0.974219</td>\n",
       "      <td>0.973146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Table     Chair       Bed\n",
       "min  0.028480  0.375552  0.403832\n",
       "max  0.739077  0.974219  0.973146"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a    b    c\n",
       "1  1.0  1.0  1.0\n",
       "2  0.0  2.0  1.0\n",
       "3  2.0  2.0  0.0\n",
       "4  2.0  0.0  2.0\n",
       "5  0.0  0.0  1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'a': [1, 3, 4, 3, 4],\n",
    "                   'b': [2, 3, 1, 2, 3],\n",
    "                   'c': [1, 5, 2, 4, 4]})\n",
    "\n",
    "df.apply(pd.value_counts).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bed</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candle</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chair</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lamp</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mirror</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a    b    c\n",
       "bed     1.0  1.0  0.0\n",
       "candle  0.0  1.0  0.0\n",
       "chair   2.0  1.0  0.0\n",
       "lamp    1.0  2.0  0.0\n",
       "mirror  0.0  0.0  5.0\n",
       "table   1.0  0.0  0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'a': ['table', 'chair', 'chair', 'lamp', 'bed'],\n",
    "                   'b': ['lamp', 'candle', 'chair', 'lamp', 'bed'],\n",
    "                   'c': ['mirror', 'mirror', 'mirror', 'mirror', 'mirror']})\n",
    "\n",
    "df.apply(pd.value_counts).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>columns</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bed</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candle</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chair</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lamp</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mirror</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "columns  a  b  c\n",
       "index           \n",
       "bed      1  1  0\n",
       "candle   0  1  0\n",
       "chair    2  1  0\n",
       "lamp     1  2  0\n",
       "mirror   0  0  5\n",
       "table    1  0  0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(**df.melt(var_name='columns', value_name='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaa</th>\n",
       "      <td>-0.264438</td>\n",
       "      <td>-1.026059</td>\n",
       "      <td>-0.619500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbb</th>\n",
       "      <td>0.927272</td>\n",
       "      <td>0.302904</td>\n",
       "      <td>-0.032399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccc</th>\n",
       "      <td>-0.264273</td>\n",
       "      <td>-0.386314</td>\n",
       "      <td>-0.217601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ddd</th>\n",
       "      <td>-0.871858</td>\n",
       "      <td>-0.348382</td>\n",
       "      <td>1.100491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            A         B         C\n",
       "aaa -0.264438 -1.026059 -0.619500\n",
       "bbb  0.927272  0.302904 -0.032399\n",
       "ccc -0.264273 -0.386314 -0.217601\n",
       "ddd -0.871858 -0.348382  1.100491"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('examples/ex3.txt', sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a,\"b\",\"c\"']\n",
      "['1,\"2\",\"3\"']\n",
      "['1,\"2\",\"3\"']\n"
     ]
    }
   ],
   "source": [
    "params = dict(lineterminator = '\\n',\n",
    "    delimiter = ';',\n",
    "    quotechar = '\"',\n",
    "    quoting = csv.QUOTE_MINIMAL)\n",
    "\n",
    "with open('examples/ex7.csv') as f:\n",
    "    reader = csv.reader(f, **params)\n",
    "    for line in reader:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(['Houston', 'Miami', 'Cleveland'])\n",
    "states_map = {'houston':'Texas', 'miami':'Florida', 'cleveland':'Ohio'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Texas\n",
       "1    Florida\n",
       "2       Ohio\n",
       "dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(lambda x: states_map[x.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = s.sample(1000000, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 ms ± 44.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s1.str.lower().map(states_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.4 ms ± 3.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit s1.map({k.title(): v for k, v in states_map.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168   -3.618197\n",
       "494    3.030996\n",
       "557    3.904664\n",
       "dtype: float64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(np.random.randn(1000))\n",
    "s[s.abs() > 3]    # use pandas methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('datasets/movielens/movies.dat', sep='::', engine='python',\n",
    "                       header=None, names=mnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Animation', \"Children's\", 'Comedy', 'Adventure', 'Fantasy',\n",
       "       'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',\n",
       "       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',\n",
       "       'Western'], dtype=object)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.genres.str.split('|', expand=True).stack().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {}\n",
    "for key, values in movies.genres.items():\n",
    "    for v in values.split('|'):\n",
    "        if v in d:\n",
    "            d[v].update({key:1})\n",
    "        else:\n",
    "            d[v] = {key:1}       \n",
    "df = movies.join(pd.DataFrame(d).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                               title                        genres  \\\n",
       "0         1                    Toy Story (1995)   Animation|Children's|Comedy   \n",
       "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy   \n",
       "2         3             Grumpier Old Men (1995)                Comedy|Romance   \n",
       "3         4            Waiting to Exhale (1995)                  Comedy|Drama   \n",
       "4         5  Father of the Bride Part II (1995)                        Comedy   \n",
       "\n",
       "   Action  Adventure  Animation  Children's  Comedy  Crime  Documentary  \\\n",
       "0     0.0        0.0        1.0         1.0     1.0    0.0          0.0   \n",
       "1     0.0        1.0        0.0         1.0     0.0    0.0          0.0   \n",
       "2     0.0        0.0        0.0         0.0     1.0    0.0          0.0   \n",
       "3     0.0        0.0        0.0         0.0     1.0    0.0          0.0   \n",
       "4     0.0        0.0        0.0         0.0     1.0    0.0          0.0   \n",
       "\n",
       "    ...     Fantasy  Film-Noir  Horror  Musical  Mystery  Romance  Sci-Fi  \\\n",
       "0   ...         0.0        0.0     0.0      0.0      0.0      0.0     0.0   \n",
       "1   ...         1.0        0.0     0.0      0.0      0.0      0.0     0.0   \n",
       "2   ...         0.0        0.0     0.0      0.0      0.0      1.0     0.0   \n",
       "3   ...         0.0        0.0     0.0      0.0      0.0      0.0     0.0   \n",
       "4   ...         0.0        0.0     0.0      0.0      0.0      0.0     0.0   \n",
       "\n",
       "   Thriller  War  Western  \n",
       "0       0.0  0.0      0.0  \n",
       "1       0.0  0.0      0.0  \n",
       "2       0.0  0.0      0.0  \n",
       "3       0.0  0.0      0.0  \n",
       "4       0.0  0.0      0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children's</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>...</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                               title                        genres  \\\n",
       "0         1                    Toy Story (1995)   Animation|Children's|Comedy   \n",
       "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy   \n",
       "2         3             Grumpier Old Men (1995)                Comedy|Romance   \n",
       "3         4            Waiting to Exhale (1995)                  Comedy|Drama   \n",
       "4         5  Father of the Bride Part II (1995)                        Comedy   \n",
       "\n",
       "   Action  Adventure  Animation  Children's  Comedy  Crime  Documentary  \\\n",
       "0       0          0          1           1       1      0            0   \n",
       "1       0          1          0           1       0      0            0   \n",
       "2       0          0          0           0       1      0            0   \n",
       "3       0          0          0           0       1      0            0   \n",
       "4       0          0          0           0       1      0            0   \n",
       "\n",
       "    ...     Fantasy  Film-Noir  Horror  Musical  Mystery  Romance  Sci-Fi  \\\n",
       "0   ...           0          0       0        0        0        0       0   \n",
       "1   ...           1          0       0        0        0        0       0   \n",
       "2   ...           0          0       0        0        0        1       0   \n",
       "3   ...           0          0       0        0        0        0       0   \n",
       "4   ...           0          0       0        0        0        0       0   \n",
       "\n",
       "   Thriller  War  Western  \n",
       "0         0    0        0  \n",
       "1         0    0        0  \n",
       "2         0    0        0  \n",
       "3         0    0        0  \n",
       "4         0    0        0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres_stacked = movies.genres.str.split('|', expand=True).stack()\n",
    "df_ind = pd.get_dummies(genres_stacked)\n",
    "df_ind_total = df_ind.groupby(level=0).sum()\n",
    "df = movies.join(df_ind_total)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a  1    10\n",
       "   2     3\n",
       "b  3     2\n",
       "   1    11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([10,3,2,11], index=[['a', 'a', 'b', 'b'],[1,2,3,1]])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10\n",
       "2     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a  1    10\n",
       "   2     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.loc[['a']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 10 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tips = pd.read_csv('examples/tips.csv')\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "      <th>tip_pct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">No</th>\n",
       "      <th>88</th>\n",
       "      <td>24.71</td>\n",
       "      <td>5.85</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "      <td>0.236746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>20.69</td>\n",
       "      <td>5.00</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>5</td>\n",
       "      <td>0.241663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10.29</td>\n",
       "      <td>2.60</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.252672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>7.51</td>\n",
       "      <td>2.00</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "      <td>0.266312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>11.61</td>\n",
       "      <td>3.39</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.291990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Yes</th>\n",
       "      <th>109</th>\n",
       "      <td>14.31</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.279525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>23.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "      <td>0.280535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>1</td>\n",
       "      <td>0.325733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9.60</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>7.25</td>\n",
       "      <td>5.15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_bill   tip smoker   day    time  size   tip_pct\n",
       "smoker                                                           \n",
       "No     88        24.71  5.85     No  Thur   Lunch     2  0.236746\n",
       "       185       20.69  5.00     No   Sun  Dinner     5  0.241663\n",
       "       51        10.29  2.60     No   Sun  Dinner     2  0.252672\n",
       "       149        7.51  2.00     No  Thur   Lunch     2  0.266312\n",
       "       232       11.61  3.39     No   Sat  Dinner     2  0.291990\n",
       "Yes    109       14.31  4.00    Yes   Sat  Dinner     2  0.279525\n",
       "       183       23.17  6.50    Yes   Sun  Dinner     4  0.280535\n",
       "       67         3.07  1.00    Yes   Sat  Dinner     1  0.325733\n",
       "       178        9.60  4.00    Yes   Sun  Dinner     2  0.416667\n",
       "       172        7.25  5.15    Yes   Sun  Dinner     2  0.710345"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.groupby('smoker').apply(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "      <th>tip_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>7.25</td>\n",
       "      <td>5.15</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>9.60</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>1</td>\n",
       "      <td>0.325733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>11.61</td>\n",
       "      <td>3.39</td>\n",
       "      <td>No</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.291990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>23.17</td>\n",
       "      <td>6.50</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "      <td>0.280535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>14.31</td>\n",
       "      <td>4.00</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Sat</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.279525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>7.51</td>\n",
       "      <td>2.00</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "      <td>0.266312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10.29</td>\n",
       "      <td>2.60</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "      <td>0.252672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>20.69</td>\n",
       "      <td>5.00</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>5</td>\n",
       "      <td>0.241663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>24.71</td>\n",
       "      <td>5.85</td>\n",
       "      <td>No</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2</td>\n",
       "      <td>0.236746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill   tip smoker   day    time  size   tip_pct\n",
       "172        7.25  5.15    Yes   Sun  Dinner     2  0.710345\n",
       "178        9.60  4.00    Yes   Sun  Dinner     2  0.416667\n",
       "67         3.07  1.00    Yes   Sat  Dinner     1  0.325733\n",
       "232       11.61  3.39     No   Sat  Dinner     2  0.291990\n",
       "183       23.17  6.50    Yes   Sun  Dinner     4  0.280535\n",
       "109       14.31  4.00    Yes   Sat  Dinner     2  0.279525\n",
       "149        7.51  2.00     No  Thur   Lunch     2  0.266312\n",
       "51        10.29  2.60     No   Sun  Dinner     2  0.252672\n",
       "185       20.69  5.00     No   Sun  Dinner     5  0.241663\n",
       "88        24.71  5.85     No  Thur   Lunch     2  0.236746"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.sort_values('tip_pct', ascending=False).groupby('smoker').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "      <th>tip</th>\n",
       "      <th>tip_pct</th>\n",
       "      <th>total_bill</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>Sat</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.291990</td>\n",
       "      <td>11.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>Thur</td>\n",
       "      <td>2</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.266312</td>\n",
       "      <td>7.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>Sun</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.252672</td>\n",
       "      <td>10.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>Sun</td>\n",
       "      <td>5</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.241663</td>\n",
       "      <td>20.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No</th>\n",
       "      <td>Thur</td>\n",
       "      <td>2</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.236746</td>\n",
       "      <td>24.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>Sun</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>5.15</td>\n",
       "      <td>0.710345</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>Sun</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>9.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>Sat</td>\n",
       "      <td>1</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.325733</td>\n",
       "      <td>3.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>Sun</td>\n",
       "      <td>4</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.280535</td>\n",
       "      <td>23.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Yes</th>\n",
       "      <td>Sat</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.279525</td>\n",
       "      <td>14.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         day  size    time   tip   tip_pct  total_bill\n",
       "smoker                                                \n",
       "No       Sat     2  Dinner  3.39  0.291990       11.61\n",
       "No      Thur     2   Lunch  2.00  0.266312        7.51\n",
       "No       Sun     2  Dinner  2.60  0.252672       10.29\n",
       "No       Sun     5  Dinner  5.00  0.241663       20.69\n",
       "No      Thur     2   Lunch  5.85  0.236746       24.71\n",
       "Yes      Sun     2  Dinner  5.15  0.710345        7.25\n",
       "Yes      Sun     2  Dinner  4.00  0.416667        9.60\n",
       "Yes      Sat     1  Dinner  1.00  0.325733        3.07\n",
       "Yes      Sun     4  Dinner  6.50  0.280535       23.17\n",
       "Yes      Sat     2  Dinner  4.00  0.279525       14.31"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.sort_values('tip_pct', ascending=False).groupby('smoker').nth(list(range(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
